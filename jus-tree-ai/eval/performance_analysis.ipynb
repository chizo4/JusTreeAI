{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static file paths.\n",
    "GROUND_TRUTH_PATH = '../../data/duo-student-finance/cases.json'\n",
    "RESULTS_PATH = '../../results/duo-student-finance/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific results files.\n",
    "result_files = [\n",
    "    'deepseek-r1:8b-0.8-no-20250129110014.json',\n",
    "    'llama3.2-0.8-no-20250129091911.json',\n",
    "    'qwen2.5:1.5b-0.8-no-20250129094820.json',\n",
    "    'deepseek-r1:8b-0.8-yes-20250129105858.json',\n",
    "    'llama3.2-0.8-yes-20250129085941.json',\n",
    "    'qwen2.5:1.5b-0.8-yes-20250129093907.json'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth data.\n",
    "with open(GROUND_TRUTH_PATH, 'r') as f:\n",
    "    ground_truth_data = json.load(f)\n",
    "\n",
    "df_truth = pd.DataFrame(ground_truth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from each file.\n",
    "df_results = {}\n",
    "\n",
    "for file in result_files:\n",
    "    result_path = os.path.join(RESULTS_PATH, file)\n",
    "    with open(result_path, 'r') as f:\n",
    "        result_data = json.load(f)\n",
    "    df_result = pd.DataFrame(result_data)\n",
    "    # Pre-process the prediction field.\n",
    "    if 'prediction' in df_result.columns:\n",
    "        df_result['prediction'] = df_result['prediction'].replace('Not Eligible', 'NotEligible')\n",
    "    # Merge the current results with the ground truth data.\n",
    "    df_merged = df_truth.merge(df_result, left_on='id', right_on='case_id', how='inner')\n",
    "    # Update the results dictionary.\n",
    "    model_key = file.rsplit('-', 1)[0]\n",
    "    model_key = model_key.replace('.json', '')\n",
    "    df_results[model_key] = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Ratio for Qwen-2.5 (no decision tree): 0.57\n"
     ]
    }
   ],
   "source": [
    "# MODEL SETUP: Qwen-2.5, params: 1.5B, temperature: 0.8, decision_tree: no.\n",
    "df_qwen_no = df_results['qwen2.5:1.5b-0.8-no']\n",
    "correct_predictions = (df_qwen_no['decision'] == df_qwen_no['prediction']).sum()\n",
    "total_cases = len(df_qwen_no)\n",
    "\n",
    "# ACCURACY RATIO: correct predictions / total cases.\n",
    "accuracy_ratio_qwen_no = correct_predictions / total_cases if total_cases > 0 else 0.0\n",
    "print(f'Accuracy Ratio for Qwen-2.5 (no decision tree): {accuracy_ratio_qwen_no:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Ratio for Qwen-2.5 (with decision tree): 0.51\n"
     ]
    }
   ],
   "source": [
    "# MODEL SETUP: Qwen-2.5, params: 1.5B, temperature: 0.8, decision_tree: yes.\n",
    "df_qwen_yes = df_results['qwen2.5:1.5b-0.8-yes']\n",
    "correct_predictions = (df_qwen_yes['decision'] == df_qwen_yes['prediction']).sum()\n",
    "total_cases = len(df_qwen_yes)\n",
    "\n",
    "# ACCURACY RATIO: correct predictions / total cases.\n",
    "accuracy_ratio_qwen_yes = correct_predictions / total_cases if total_cases > 0 else 0.0\n",
    "print(f'Accuracy Ratio for Qwen-2.5 (with decision tree): {accuracy_ratio_qwen_yes:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Ratio for LLaMa-3.2 (no decision tree): 0.56\n"
     ]
    }
   ],
   "source": [
    "# MODEL SETUP: LLaMa-3.2, params: 3.2B, temperature: 0.8, decision_tree: no.\n",
    "df_llama_no = df_results['llama3.2-0.8-no']\n",
    "correct_predictions = (df_llama_no['decision'] == df_llama_no['prediction']).sum()\n",
    "total_cases = len(df_llama_no)\n",
    "\n",
    "# ACCURACY RATIO: correct predictions / total cases.\n",
    "accuracy_ratio_llama_no = correct_predictions / total_cases if total_cases > 0 else 0.0\n",
    "print(f'Accuracy Ratio for LLaMa-3.2 (no decision tree): {accuracy_ratio_llama_no:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Ratio for LLaMa-3.2 (with decision tree): 0.59\n"
     ]
    }
   ],
   "source": [
    "# MODEL SETUP: LLaMa-3.2, params: 3.2B, temperature: 0.8, decision_tree: yes.\n",
    "df_llama_yes = df_results['llama3.2-0.8-yes']\n",
    "correct_predictions = (df_llama_yes['decision'] == df_llama_yes['prediction']).sum()\n",
    "total_cases = len(df_llama_yes)\n",
    "\n",
    "# ACCURACY RATIO: correct predictions / total cases.\n",
    "accuracy_ratio_llama_yes = correct_predictions / total_cases if total_cases > 0 else 0.0\n",
    "print(f'Accuracy Ratio for LLaMa-3.2 (with decision tree): {accuracy_ratio_llama_yes:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Ratio for DeepSeek-R1 (no decision tree): 0.68\n"
     ]
    }
   ],
   "source": [
    "# MODEL SETUP: DeepSeek-R1, params: 8B, temperature: 0.8, decision_tree: no.\n",
    "df_deepseek_no = df_results['deepseek-r1:8b-0.8-no']\n",
    "correct_predictions = (df_deepseek_no['decision'] == df_deepseek_no['prediction']).sum()\n",
    "total_cases = len(df_deepseek_no)\n",
    "\n",
    "# ACCURACY RATIO: correct predictions / total cases.\n",
    "accuracy_ratio_deepseek_no = correct_predictions / total_cases if total_cases > 0 else 0.0\n",
    "print(f'Accuracy Ratio for DeepSeek-R1 (no decision tree): {accuracy_ratio_deepseek_no:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Ratio for DeepSeek-R1 (with decision tree): 0.87\n"
     ]
    }
   ],
   "source": [
    "# MODEL SETUP: DeepSeek-R1, params: 8B, temperature: 0.8, decision_tree: yes.\n",
    "df_deepseek_yes = df_results['deepseek-r1:8b-0.8-yes']\n",
    "correct_predictions = (df_deepseek_yes['decision'] == df_deepseek_yes['prediction']).sum()\n",
    "total_cases = len(df_deepseek_yes)\n",
    "\n",
    "# ACCURACY RATIO: correct predictions / total cases.\n",
    "accuracy_ratio_deepseek_yes = correct_predictions / total_cases if total_cases > 0 else 0.0\n",
    "print(f'Accuracy Ratio for DeepSeek-R1 (with decision tree): {accuracy_ratio_deepseek_yes:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
