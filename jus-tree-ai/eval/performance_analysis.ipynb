{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static file paths.\n",
    "GROUND_TRUTH_PATH = '../../data/duo-student-finance/cases.json'\n",
    "RESULTS_PATH = '../../results/duo-student-finance/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific results files.\n",
    "result_files = [\n",
    "    'deepseek-r1:8b-0.8-no-20250129110014.json',\n",
    "    'llama3.2-0.8-no-20250129091911.json',\n",
    "    'qwen2.5:1.5b-0.8-no-20250129094820.json',\n",
    "    'deepseek-r1:8b-0.8-yes-20250129105858.json',\n",
    "    'llama3.2-0.8-yes-20250129085941.json',\n",
    "    'qwen2.5:1.5b-0.8-yes-20250129093907.json'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth data.\n",
    "with open(GROUND_TRUTH_PATH, 'r') as f:\n",
    "    ground_truth_data = json.load(f)\n",
    "\n",
    "df_truth = pd.DataFrame(ground_truth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from each file.\n",
    "df_results = {}\n",
    "\n",
    "for file in result_files:\n",
    "    result_path = os.path.join(RESULTS_PATH, file)\n",
    "    with open(result_path, 'r') as f:\n",
    "        result_data = json.load(f)\n",
    "    df_result = pd.DataFrame(result_data)\n",
    "    # Pre-process the prediction field.\n",
    "    if 'prediction' in df_result.columns:\n",
    "        df_result['prediction'] = df_result['prediction'].replace('Not Eligible', 'NotEligible')\n",
    "    # Merge the current results with the ground truth data.\n",
    "    df_merged = df_truth.merge(df_result, left_on='id', right_on='case_id', how='inner')\n",
    "    # Update the results dictionary.\n",
    "    model_key = file.rsplit('-', 1)[0]\n",
    "    model_key = model_key.replace('.json', '')\n",
    "    df_results[model_key] = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Ratio for Qwen-2.5 (no decision tree): 0.57\n"
     ]
    }
   ],
   "source": [
    "# MODEL SETUP: Qwen-2.5, params: 1.5B, temperature: 0.8, decision_tree: no.\n",
    "df_qwen_no = df_results['qwen2.5:1.5b-0.8-no']\n",
    "correct_predictions = (df_qwen_no['decision'] == df_qwen_no['prediction']).sum()\n",
    "total_cases = len(df_qwen_no)\n",
    "\n",
    "# ACCURACY RATIO: correct predictions / total cases.\n",
    "accuracy = correct_predictions / total_cases if total_cases > 0 else 0.0\n",
    "df_results['qwen2.5:1.5b-0.8-no']['accuracy'] = accuracy\n",
    "print(f'Accuracy Ratio for Qwen-2.5 (no decision tree): {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Ratio for Qwen-2.5 (with decision tree): 0.51\n"
     ]
    }
   ],
   "source": [
    "# MODEL SETUP: Qwen-2.5, params: 1.5B, temperature: 0.8, decision_tree: yes.\n",
    "df_qwen_yes = df_results['qwen2.5:1.5b-0.8-yes']\n",
    "correct_predictions = (df_qwen_yes['decision'] == df_qwen_yes['prediction']).sum()\n",
    "total_cases = len(df_qwen_yes)\n",
    "\n",
    "# ACCURACY RATIO: correct predictions / total cases.\n",
    "accuracy_ratio_qwen_yes = correct_predictions / total_cases if total_cases > 0 else 0.0\n",
    "df_results['qwen2.5:1.5b-0.8-yes']['accuracy'] = accuracy_ratio_qwen_yes\n",
    "print(f'Accuracy Ratio for Qwen-2.5 (with decision tree): {accuracy_ratio_qwen_yes:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Ratio for LLaMa-3.2 (no decision tree): 0.56\n"
     ]
    }
   ],
   "source": [
    "# MODEL SETUP: LLaMa-3.2, params: 3.2B, temperature: 0.8, decision_tree: no.\n",
    "df_llama_no = df_results['llama3.2-0.8-no']\n",
    "correct_predictions = (df_llama_no['decision'] == df_llama_no['prediction']).sum()\n",
    "total_cases = len(df_llama_no)\n",
    "\n",
    "# ACCURACY RATIO: correct predictions / total cases.\n",
    "accuracy_ratio_llama_no = correct_predictions / total_cases if total_cases > 0 else 0.0\n",
    "df_results['llama3.2-0.8-no']['accuracy'] = accuracy_ratio_llama_no\n",
    "print(f'Accuracy Ratio for LLaMa-3.2 (no decision tree): {accuracy_ratio_llama_no:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Ratio for LLaMa-3.2 (with decision tree): 0.59\n"
     ]
    }
   ],
   "source": [
    "# MODEL SETUP: LLaMa-3.2, params: 3.2B, temperature: 0.8, decision_tree: yes.\n",
    "df_llama_yes = df_results['llama3.2-0.8-yes']\n",
    "correct_predictions = (df_llama_yes['decision'] == df_llama_yes['prediction']).sum()\n",
    "total_cases = len(df_llama_yes)\n",
    "\n",
    "# ACCURACY RATIO: correct predictions / total cases.\n",
    "accuracy_ratio_llama_yes = correct_predictions / total_cases if total_cases > 0 else 0.0\n",
    "df_results['llama3.2-0.8-yes']['accuracy'] = accuracy_ratio_llama_yes\n",
    "print(f'Accuracy Ratio for LLaMa-3.2 (with decision tree): {accuracy_ratio_llama_yes:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Ratio for DeepSeek-R1 (no decision tree): 0.68\n"
     ]
    }
   ],
   "source": [
    "# MODEL SETUP: DeepSeek-R1, params: 8B, temperature: 0.8, decision_tree: no.\n",
    "df_deepseek_no = df_results['deepseek-r1:8b-0.8-no']\n",
    "correct_predictions = (df_deepseek_no['decision'] == df_deepseek_no['prediction']).sum()\n",
    "total_cases = len(df_deepseek_no)\n",
    "\n",
    "# ACCURACY RATIO: correct predictions / total cases.\n",
    "accuracy_ratio_deepseek_no = correct_predictions / total_cases if total_cases > 0 else 0.0\n",
    "df_results['deepseek-r1:8b-0.8-no']['accuracy'] = accuracy_ratio_deepseek_no\n",
    "print(f'Accuracy Ratio for DeepSeek-R1 (no decision tree): {accuracy_ratio_deepseek_no:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Ratio for DeepSeek-R1 (with decision tree): 0.87\n"
     ]
    }
   ],
   "source": [
    "# MODEL SETUP: DeepSeek-R1, params: 8B, temperature: 0.8, decision_tree: yes.\n",
    "df_deepseek_yes = df_results['deepseek-r1:8b-0.8-yes']\n",
    "correct_predictions = (df_deepseek_yes['decision'] == df_deepseek_yes['prediction']).sum()\n",
    "total_cases = len(df_deepseek_yes)\n",
    "\n",
    "# ACCURACY RATIO: correct predictions / total cases.\n",
    "accuracy_ratio_deepseek_yes = correct_predictions / total_cases if total_cases > 0 else 0.0\n",
    "df_results['deepseek-r1:8b-0.8-yes']['accuracy'] = accuracy_ratio_deepseek_yes\n",
    "print(f'Accuracy Ratio for DeepSeek-R1 (with decision tree): {accuracy_ratio_deepseek_yes:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PERFORMANCE PLOT\n",
    "SAVE_PATH = 'resources/plots/model_performance2.png'\n",
    "\n",
    "# Load data.\n",
    "models = ['Qwen 2.5 (1.5B)', 'LLaMA 3.2 (3.2B)', 'DeepSeek-R1 (8.0B)']\n",
    "accuracy_model_only = [\n",
    "    df_results['qwen2.5:1.5b-0.8-no']['accuracy'][0], \n",
    "    df_results['llama3.2-0.8-no']['accuracy'][0], \n",
    "    df_results['deepseek-r1:8b-0.8-no']['accuracy'][0]\n",
    "]\n",
    "accuracy_model_tree = [\n",
    "    df_results['qwen2.5:1.5b-0.8-yes']['accuracy'][0], \n",
    "    df_results['llama3.2-0.8-yes']['accuracy'][0], \n",
    "    df_results['deepseek-r1:8b-0.8-yes']['accuracy'][0]\n",
    "]\n",
    "\n",
    "# Setup bars.\n",
    "x = np.arange(len(models))\n",
    "width = 0.3\n",
    "# Colors: Qwen (red), LLaMA (blue), DeepSeek (green).\n",
    "colors = ['red', 'blue', 'green']\n",
    "# Create the plot.\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "# Adjust effects for the bars.\n",
    "bars1 = ax.bar(\n",
    "    x - width/2,\n",
    "    accuracy_model_only,\n",
    "    width,\n",
    "    color=colors,\n",
    "    alpha=0.6,\n",
    "    label='Model-Only',\n",
    "    edgecolor='black',\n",
    "    hatch=''\n",
    ")\n",
    "bars2 = ax.bar(\n",
    "    x + width/2,\n",
    "    accuracy_model_tree,\n",
    "    width,\n",
    "    color=colors,\n",
    "    alpha=1.0,\n",
    "    label='Model + Decision Tree',\n",
    "    edgecolor='black',\n",
    "    hatch='///'\n",
    ")\n",
    "# Label bars.\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width()/2,\n",
    "            height + 0.02,\n",
    "            f'{height:.2f}',\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontsize=11,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "# Label axes.\n",
    "ax.set_ylabel('Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Models', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Model Performance (with/without Decision Tree)', fontsize=18, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, fontsize=12, fontweight='bold')\n",
    "# Legend.\n",
    "ax.legend(fontsize=11, loc='upper left')\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "# Render the plot and save itsc copy.\n",
    "plt.ylim(0.4, 1.0)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(SAVE_PATH, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
